{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Multi-Channel CNN for  Exo-Detect CNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "CAYF4aW7mu5J",
        "colab_type": "code",
        "outputId": "e2db5df5-0f59-40e7-b904-277bdee63f88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMKNP0T1mwmG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers import Flatten, Dense, Dropout, BatchNormalization, Conv1D, Conv2D, MaxPooling1D, MaxPooling2D"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLKH81Bfm_fE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.optimizers import Adam, SGD"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5NdYHfKunCty",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0aK1yiNnHlZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLIRT7gznO6Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler, normalize"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77DtYpcQqeba",
        "colab_type": "code",
        "outputId": "9828fde4-9a17-4c88-8bbb-fd747ed05130",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "data = pd.read_csv('/content/drive/My Drive/ExoTrain.csv')\n",
        "data.shape"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3960, 3198)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swDMonRjm0KI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_data = pd.read_csv('/content/drive/My Drive/exoTest.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOikNcSEqkXj",
        "colab_type": "code",
        "outputId": "c289e88b-26b4-46de-e3c0-16b641c059d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        }
      },
      "source": [
        "data.head()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>LABEL</th>\n",
              "      <th>FLUX.1</th>\n",
              "      <th>FLUX.2</th>\n",
              "      <th>FLUX.3</th>\n",
              "      <th>FLUX.4</th>\n",
              "      <th>FLUX.5</th>\n",
              "      <th>FLUX.6</th>\n",
              "      <th>FLUX.7</th>\n",
              "      <th>FLUX.8</th>\n",
              "      <th>FLUX.9</th>\n",
              "      <th>FLUX.10</th>\n",
              "      <th>FLUX.11</th>\n",
              "      <th>FLUX.12</th>\n",
              "      <th>FLUX.13</th>\n",
              "      <th>FLUX.14</th>\n",
              "      <th>FLUX.15</th>\n",
              "      <th>FLUX.16</th>\n",
              "      <th>FLUX.17</th>\n",
              "      <th>FLUX.18</th>\n",
              "      <th>FLUX.19</th>\n",
              "      <th>FLUX.20</th>\n",
              "      <th>FLUX.21</th>\n",
              "      <th>FLUX.22</th>\n",
              "      <th>FLUX.23</th>\n",
              "      <th>FLUX.24</th>\n",
              "      <th>FLUX.25</th>\n",
              "      <th>FLUX.26</th>\n",
              "      <th>FLUX.27</th>\n",
              "      <th>FLUX.28</th>\n",
              "      <th>FLUX.29</th>\n",
              "      <th>FLUX.30</th>\n",
              "      <th>FLUX.31</th>\n",
              "      <th>FLUX.32</th>\n",
              "      <th>FLUX.33</th>\n",
              "      <th>FLUX.34</th>\n",
              "      <th>FLUX.35</th>\n",
              "      <th>FLUX.36</th>\n",
              "      <th>FLUX.37</th>\n",
              "      <th>FLUX.38</th>\n",
              "      <th>FLUX.39</th>\n",
              "      <th>...</th>\n",
              "      <th>FLUX.3158</th>\n",
              "      <th>FLUX.3159</th>\n",
              "      <th>FLUX.3160</th>\n",
              "      <th>FLUX.3161</th>\n",
              "      <th>FLUX.3162</th>\n",
              "      <th>FLUX.3163</th>\n",
              "      <th>FLUX.3164</th>\n",
              "      <th>FLUX.3165</th>\n",
              "      <th>FLUX.3166</th>\n",
              "      <th>FLUX.3167</th>\n",
              "      <th>FLUX.3168</th>\n",
              "      <th>FLUX.3169</th>\n",
              "      <th>FLUX.3170</th>\n",
              "      <th>FLUX.3171</th>\n",
              "      <th>FLUX.3172</th>\n",
              "      <th>FLUX.3173</th>\n",
              "      <th>FLUX.3174</th>\n",
              "      <th>FLUX.3175</th>\n",
              "      <th>FLUX.3176</th>\n",
              "      <th>FLUX.3177</th>\n",
              "      <th>FLUX.3178</th>\n",
              "      <th>FLUX.3179</th>\n",
              "      <th>FLUX.3180</th>\n",
              "      <th>FLUX.3181</th>\n",
              "      <th>FLUX.3182</th>\n",
              "      <th>FLUX.3183</th>\n",
              "      <th>FLUX.3184</th>\n",
              "      <th>FLUX.3185</th>\n",
              "      <th>FLUX.3186</th>\n",
              "      <th>FLUX.3187</th>\n",
              "      <th>FLUX.3188</th>\n",
              "      <th>FLUX.3189</th>\n",
              "      <th>FLUX.3190</th>\n",
              "      <th>FLUX.3191</th>\n",
              "      <th>FLUX.3192</th>\n",
              "      <th>FLUX.3193</th>\n",
              "      <th>FLUX.3194</th>\n",
              "      <th>FLUX.3195</th>\n",
              "      <th>FLUX.3196</th>\n",
              "      <th>FLUX.3197</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2</td>\n",
              "      <td>-38.88</td>\n",
              "      <td>-33.83</td>\n",
              "      <td>-58.54</td>\n",
              "      <td>-40.09</td>\n",
              "      <td>-79.31</td>\n",
              "      <td>-72.81</td>\n",
              "      <td>-86.55</td>\n",
              "      <td>-85.33</td>\n",
              "      <td>-83.97</td>\n",
              "      <td>-73.38</td>\n",
              "      <td>-86.51</td>\n",
              "      <td>-74.97</td>\n",
              "      <td>-73.15</td>\n",
              "      <td>-86.13</td>\n",
              "      <td>-76.57</td>\n",
              "      <td>-61.27</td>\n",
              "      <td>-37.23</td>\n",
              "      <td>-48.53</td>\n",
              "      <td>-30.96</td>\n",
              "      <td>-8.14</td>\n",
              "      <td>-5.54</td>\n",
              "      <td>15.79</td>\n",
              "      <td>45.71</td>\n",
              "      <td>10.61</td>\n",
              "      <td>40.66</td>\n",
              "      <td>16.70</td>\n",
              "      <td>15.18</td>\n",
              "      <td>11.98</td>\n",
              "      <td>-203.70</td>\n",
              "      <td>19.13</td>\n",
              "      <td>19.13</td>\n",
              "      <td>19.13</td>\n",
              "      <td>19.13</td>\n",
              "      <td>19.13</td>\n",
              "      <td>17.02</td>\n",
              "      <td>-8.50</td>\n",
              "      <td>-13.87</td>\n",
              "      <td>-29.10</td>\n",
              "      <td>-34.29</td>\n",
              "      <td>...</td>\n",
              "      <td>-36.75</td>\n",
              "      <td>-15.49</td>\n",
              "      <td>-13.24</td>\n",
              "      <td>20.46</td>\n",
              "      <td>-1.47</td>\n",
              "      <td>-0.40</td>\n",
              "      <td>27.80</td>\n",
              "      <td>-58.20</td>\n",
              "      <td>-58.20</td>\n",
              "      <td>-72.04</td>\n",
              "      <td>-58.01</td>\n",
              "      <td>-30.92</td>\n",
              "      <td>-13.42</td>\n",
              "      <td>-13.98</td>\n",
              "      <td>-5.43</td>\n",
              "      <td>8.71</td>\n",
              "      <td>1.80</td>\n",
              "      <td>36.59</td>\n",
              "      <td>-9.80</td>\n",
              "      <td>-19.53</td>\n",
              "      <td>-19.53</td>\n",
              "      <td>-24.32</td>\n",
              "      <td>-23.88</td>\n",
              "      <td>-33.07</td>\n",
              "      <td>-9.03</td>\n",
              "      <td>3.75</td>\n",
              "      <td>11.61</td>\n",
              "      <td>-12.66</td>\n",
              "      <td>-5.69</td>\n",
              "      <td>12.53</td>\n",
              "      <td>-3.28</td>\n",
              "      <td>-32.21</td>\n",
              "      <td>-32.21</td>\n",
              "      <td>-24.89</td>\n",
              "      <td>-4.86</td>\n",
              "      <td>0.76</td>\n",
              "      <td>-11.70</td>\n",
              "      <td>6.46</td>\n",
              "      <td>16.00</td>\n",
              "      <td>19.93</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>532.64</td>\n",
              "      <td>535.92</td>\n",
              "      <td>513.73</td>\n",
              "      <td>496.92</td>\n",
              "      <td>456.45</td>\n",
              "      <td>466.00</td>\n",
              "      <td>464.50</td>\n",
              "      <td>486.39</td>\n",
              "      <td>436.56</td>\n",
              "      <td>484.39</td>\n",
              "      <td>469.66</td>\n",
              "      <td>462.30</td>\n",
              "      <td>492.23</td>\n",
              "      <td>441.20</td>\n",
              "      <td>483.17</td>\n",
              "      <td>481.28</td>\n",
              "      <td>535.31</td>\n",
              "      <td>554.34</td>\n",
              "      <td>562.80</td>\n",
              "      <td>540.14</td>\n",
              "      <td>576.34</td>\n",
              "      <td>551.67</td>\n",
              "      <td>556.69</td>\n",
              "      <td>550.86</td>\n",
              "      <td>577.33</td>\n",
              "      <td>562.08</td>\n",
              "      <td>577.97</td>\n",
              "      <td>530.67</td>\n",
              "      <td>553.27</td>\n",
              "      <td>538.33</td>\n",
              "      <td>527.17</td>\n",
              "      <td>532.50</td>\n",
              "      <td>273.66</td>\n",
              "      <td>273.66</td>\n",
              "      <td>292.39</td>\n",
              "      <td>298.44</td>\n",
              "      <td>252.64</td>\n",
              "      <td>233.58</td>\n",
              "      <td>171.41</td>\n",
              "      <td>...</td>\n",
              "      <td>-51.09</td>\n",
              "      <td>-33.30</td>\n",
              "      <td>-61.53</td>\n",
              "      <td>-89.61</td>\n",
              "      <td>-69.17</td>\n",
              "      <td>-86.47</td>\n",
              "      <td>-140.91</td>\n",
              "      <td>-84.20</td>\n",
              "      <td>-84.20</td>\n",
              "      <td>-89.09</td>\n",
              "      <td>-55.44</td>\n",
              "      <td>-61.05</td>\n",
              "      <td>-29.17</td>\n",
              "      <td>-63.80</td>\n",
              "      <td>-57.61</td>\n",
              "      <td>2.70</td>\n",
              "      <td>-31.25</td>\n",
              "      <td>-47.09</td>\n",
              "      <td>-6.53</td>\n",
              "      <td>14.00</td>\n",
              "      <td>14.00</td>\n",
              "      <td>-25.05</td>\n",
              "      <td>-34.98</td>\n",
              "      <td>-32.08</td>\n",
              "      <td>-17.06</td>\n",
              "      <td>-27.77</td>\n",
              "      <td>7.86</td>\n",
              "      <td>-70.77</td>\n",
              "      <td>-64.44</td>\n",
              "      <td>-83.83</td>\n",
              "      <td>-71.69</td>\n",
              "      <td>13.31</td>\n",
              "      <td>13.31</td>\n",
              "      <td>-29.89</td>\n",
              "      <td>-20.88</td>\n",
              "      <td>5.06</td>\n",
              "      <td>-11.80</td>\n",
              "      <td>-28.91</td>\n",
              "      <td>-70.02</td>\n",
              "      <td>-96.67</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>326.52</td>\n",
              "      <td>347.39</td>\n",
              "      <td>302.35</td>\n",
              "      <td>298.13</td>\n",
              "      <td>317.74</td>\n",
              "      <td>312.70</td>\n",
              "      <td>322.33</td>\n",
              "      <td>311.31</td>\n",
              "      <td>312.42</td>\n",
              "      <td>323.33</td>\n",
              "      <td>311.14</td>\n",
              "      <td>326.19</td>\n",
              "      <td>313.11</td>\n",
              "      <td>313.89</td>\n",
              "      <td>317.96</td>\n",
              "      <td>330.92</td>\n",
              "      <td>341.10</td>\n",
              "      <td>360.58</td>\n",
              "      <td>370.29</td>\n",
              "      <td>369.71</td>\n",
              "      <td>339.00</td>\n",
              "      <td>336.24</td>\n",
              "      <td>319.31</td>\n",
              "      <td>321.56</td>\n",
              "      <td>308.02</td>\n",
              "      <td>296.82</td>\n",
              "      <td>279.34</td>\n",
              "      <td>275.78</td>\n",
              "      <td>289.67</td>\n",
              "      <td>281.33</td>\n",
              "      <td>285.37</td>\n",
              "      <td>281.87</td>\n",
              "      <td>88.75</td>\n",
              "      <td>88.75</td>\n",
              "      <td>67.71</td>\n",
              "      <td>74.46</td>\n",
              "      <td>69.34</td>\n",
              "      <td>76.51</td>\n",
              "      <td>80.26</td>\n",
              "      <td>...</td>\n",
              "      <td>-2.75</td>\n",
              "      <td>14.29</td>\n",
              "      <td>-14.18</td>\n",
              "      <td>-25.14</td>\n",
              "      <td>-13.43</td>\n",
              "      <td>-14.74</td>\n",
              "      <td>2.24</td>\n",
              "      <td>-31.07</td>\n",
              "      <td>-31.07</td>\n",
              "      <td>-50.27</td>\n",
              "      <td>-39.22</td>\n",
              "      <td>-51.33</td>\n",
              "      <td>-18.53</td>\n",
              "      <td>-1.99</td>\n",
              "      <td>10.43</td>\n",
              "      <td>-1.97</td>\n",
              "      <td>-15.32</td>\n",
              "      <td>-23.38</td>\n",
              "      <td>-27.71</td>\n",
              "      <td>-36.12</td>\n",
              "      <td>-36.12</td>\n",
              "      <td>-15.65</td>\n",
              "      <td>6.63</td>\n",
              "      <td>10.66</td>\n",
              "      <td>-8.57</td>\n",
              "      <td>-8.29</td>\n",
              "      <td>-21.90</td>\n",
              "      <td>-25.80</td>\n",
              "      <td>-29.86</td>\n",
              "      <td>7.42</td>\n",
              "      <td>5.71</td>\n",
              "      <td>-3.73</td>\n",
              "      <td>-3.73</td>\n",
              "      <td>30.05</td>\n",
              "      <td>20.03</td>\n",
              "      <td>-12.67</td>\n",
              "      <td>-8.77</td>\n",
              "      <td>-17.31</td>\n",
              "      <td>-17.35</td>\n",
              "      <td>13.98</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>-1107.21</td>\n",
              "      <td>-1112.59</td>\n",
              "      <td>-1118.95</td>\n",
              "      <td>-1095.10</td>\n",
              "      <td>-1057.55</td>\n",
              "      <td>-1034.48</td>\n",
              "      <td>-998.34</td>\n",
              "      <td>-1022.71</td>\n",
              "      <td>-989.57</td>\n",
              "      <td>-970.88</td>\n",
              "      <td>-933.30</td>\n",
              "      <td>-889.49</td>\n",
              "      <td>-888.66</td>\n",
              "      <td>-853.95</td>\n",
              "      <td>-800.91</td>\n",
              "      <td>-754.48</td>\n",
              "      <td>-717.24</td>\n",
              "      <td>-649.34</td>\n",
              "      <td>-605.71</td>\n",
              "      <td>-575.62</td>\n",
              "      <td>-526.37</td>\n",
              "      <td>-490.12</td>\n",
              "      <td>-458.73</td>\n",
              "      <td>-447.76</td>\n",
              "      <td>-419.54</td>\n",
              "      <td>-410.76</td>\n",
              "      <td>-404.10</td>\n",
              "      <td>-425.38</td>\n",
              "      <td>-397.29</td>\n",
              "      <td>-412.73</td>\n",
              "      <td>-446.49</td>\n",
              "      <td>-413.46</td>\n",
              "      <td>-1006.21</td>\n",
              "      <td>-1006.21</td>\n",
              "      <td>-973.29</td>\n",
              "      <td>-986.01</td>\n",
              "      <td>-975.88</td>\n",
              "      <td>-982.20</td>\n",
              "      <td>-953.73</td>\n",
              "      <td>...</td>\n",
              "      <td>-694.76</td>\n",
              "      <td>-705.01</td>\n",
              "      <td>-625.24</td>\n",
              "      <td>-604.16</td>\n",
              "      <td>-668.26</td>\n",
              "      <td>-742.18</td>\n",
              "      <td>-820.55</td>\n",
              "      <td>-874.76</td>\n",
              "      <td>-874.76</td>\n",
              "      <td>-853.68</td>\n",
              "      <td>-808.62</td>\n",
              "      <td>-777.88</td>\n",
              "      <td>-712.62</td>\n",
              "      <td>-694.01</td>\n",
              "      <td>-655.74</td>\n",
              "      <td>-599.74</td>\n",
              "      <td>-617.30</td>\n",
              "      <td>-602.98</td>\n",
              "      <td>-539.29</td>\n",
              "      <td>-672.71</td>\n",
              "      <td>-672.71</td>\n",
              "      <td>-594.49</td>\n",
              "      <td>-597.60</td>\n",
              "      <td>-560.77</td>\n",
              "      <td>-501.95</td>\n",
              "      <td>-461.62</td>\n",
              "      <td>-468.59</td>\n",
              "      <td>-513.24</td>\n",
              "      <td>-504.70</td>\n",
              "      <td>-521.95</td>\n",
              "      <td>-594.37</td>\n",
              "      <td>-401.66</td>\n",
              "      <td>-401.66</td>\n",
              "      <td>-357.24</td>\n",
              "      <td>-443.76</td>\n",
              "      <td>-438.54</td>\n",
              "      <td>-399.71</td>\n",
              "      <td>-384.65</td>\n",
              "      <td>-411.79</td>\n",
              "      <td>-510.54</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "      <td>211.10</td>\n",
              "      <td>163.57</td>\n",
              "      <td>179.16</td>\n",
              "      <td>187.82</td>\n",
              "      <td>188.46</td>\n",
              "      <td>168.13</td>\n",
              "      <td>203.46</td>\n",
              "      <td>178.65</td>\n",
              "      <td>166.49</td>\n",
              "      <td>139.34</td>\n",
              "      <td>146.76</td>\n",
              "      <td>149.16</td>\n",
              "      <td>162.55</td>\n",
              "      <td>159.35</td>\n",
              "      <td>173.32</td>\n",
              "      <td>108.24</td>\n",
              "      <td>175.35</td>\n",
              "      <td>143.40</td>\n",
              "      <td>119.30</td>\n",
              "      <td>168.29</td>\n",
              "      <td>111.73</td>\n",
              "      <td>123.88</td>\n",
              "      <td>111.99</td>\n",
              "      <td>95.35</td>\n",
              "      <td>139.62</td>\n",
              "      <td>127.99</td>\n",
              "      <td>66.95</td>\n",
              "      <td>90.45</td>\n",
              "      <td>63.15</td>\n",
              "      <td>64.90</td>\n",
              "      <td>-5.26</td>\n",
              "      <td>21.18</td>\n",
              "      <td>112.74</td>\n",
              "      <td>112.74</td>\n",
              "      <td>101.49</td>\n",
              "      <td>119.48</td>\n",
              "      <td>96.91</td>\n",
              "      <td>120.71</td>\n",
              "      <td>66.15</td>\n",
              "      <td>...</td>\n",
              "      <td>-3.63</td>\n",
              "      <td>3.73</td>\n",
              "      <td>-18.59</td>\n",
              "      <td>-62.30</td>\n",
              "      <td>-73.95</td>\n",
              "      <td>-112.23</td>\n",
              "      <td>-146.48</td>\n",
              "      <td>-76.82</td>\n",
              "      <td>-76.82</td>\n",
              "      <td>-68.02</td>\n",
              "      <td>-34.60</td>\n",
              "      <td>-71.05</td>\n",
              "      <td>-36.38</td>\n",
              "      <td>-6.16</td>\n",
              "      <td>-21.48</td>\n",
              "      <td>-57.98</td>\n",
              "      <td>-55.37</td>\n",
              "      <td>-64.99</td>\n",
              "      <td>-73.77</td>\n",
              "      <td>-33.41</td>\n",
              "      <td>-33.41</td>\n",
              "      <td>12.84</td>\n",
              "      <td>-22.15</td>\n",
              "      <td>-19.23</td>\n",
              "      <td>-8.40</td>\n",
              "      <td>22.04</td>\n",
              "      <td>-9.98</td>\n",
              "      <td>1.12</td>\n",
              "      <td>-49.16</td>\n",
              "      <td>-69.34</td>\n",
              "      <td>-98.45</td>\n",
              "      <td>30.34</td>\n",
              "      <td>30.34</td>\n",
              "      <td>29.62</td>\n",
              "      <td>28.80</td>\n",
              "      <td>19.27</td>\n",
              "      <td>-43.90</td>\n",
              "      <td>-41.63</td>\n",
              "      <td>-52.90</td>\n",
              "      <td>-16.16</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 3198 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   LABEL   FLUX.1   FLUX.2   FLUX.3  ...  FLUX.3194  FLUX.3195  FLUX.3196  FLUX.3197\n",
              "0      2   -38.88   -33.83   -58.54  ...     -11.70       6.46      16.00      19.93\n",
              "1      2   532.64   535.92   513.73  ...     -11.80     -28.91     -70.02     -96.67\n",
              "2      2   326.52   347.39   302.35  ...      -8.77     -17.31     -17.35      13.98\n",
              "3      2 -1107.21 -1112.59 -1118.95  ...    -399.71    -384.65    -411.79    -510.54\n",
              "4      2   211.10   163.57   179.16  ...     -43.90     -41.63     -52.90     -16.16\n",
              "\n",
              "[5 rows x 3198 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ubL3m1aoe41",
        "colab_type": "code",
        "outputId": "5190adc4-a8cf-448c-e611-f273484bd8bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        }
      },
      "source": [
        "test_data.head()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>LABEL</th>\n",
              "      <th>FLUX.1</th>\n",
              "      <th>FLUX.2</th>\n",
              "      <th>FLUX.3</th>\n",
              "      <th>FLUX.4</th>\n",
              "      <th>FLUX.5</th>\n",
              "      <th>FLUX.6</th>\n",
              "      <th>FLUX.7</th>\n",
              "      <th>FLUX.8</th>\n",
              "      <th>FLUX.9</th>\n",
              "      <th>FLUX.10</th>\n",
              "      <th>FLUX.11</th>\n",
              "      <th>FLUX.12</th>\n",
              "      <th>FLUX.13</th>\n",
              "      <th>FLUX.14</th>\n",
              "      <th>FLUX.15</th>\n",
              "      <th>FLUX.16</th>\n",
              "      <th>FLUX.17</th>\n",
              "      <th>FLUX.18</th>\n",
              "      <th>FLUX.19</th>\n",
              "      <th>FLUX.20</th>\n",
              "      <th>FLUX.21</th>\n",
              "      <th>FLUX.22</th>\n",
              "      <th>FLUX.23</th>\n",
              "      <th>FLUX.24</th>\n",
              "      <th>FLUX.25</th>\n",
              "      <th>FLUX.26</th>\n",
              "      <th>FLUX.27</th>\n",
              "      <th>FLUX.28</th>\n",
              "      <th>FLUX.29</th>\n",
              "      <th>FLUX.30</th>\n",
              "      <th>FLUX.31</th>\n",
              "      <th>FLUX.32</th>\n",
              "      <th>FLUX.33</th>\n",
              "      <th>FLUX.34</th>\n",
              "      <th>FLUX.35</th>\n",
              "      <th>FLUX.36</th>\n",
              "      <th>FLUX.37</th>\n",
              "      <th>FLUX.38</th>\n",
              "      <th>FLUX.39</th>\n",
              "      <th>...</th>\n",
              "      <th>FLUX.3158</th>\n",
              "      <th>FLUX.3159</th>\n",
              "      <th>FLUX.3160</th>\n",
              "      <th>FLUX.3161</th>\n",
              "      <th>FLUX.3162</th>\n",
              "      <th>FLUX.3163</th>\n",
              "      <th>FLUX.3164</th>\n",
              "      <th>FLUX.3165</th>\n",
              "      <th>FLUX.3166</th>\n",
              "      <th>FLUX.3167</th>\n",
              "      <th>FLUX.3168</th>\n",
              "      <th>FLUX.3169</th>\n",
              "      <th>FLUX.3170</th>\n",
              "      <th>FLUX.3171</th>\n",
              "      <th>FLUX.3172</th>\n",
              "      <th>FLUX.3173</th>\n",
              "      <th>FLUX.3174</th>\n",
              "      <th>FLUX.3175</th>\n",
              "      <th>FLUX.3176</th>\n",
              "      <th>FLUX.3177</th>\n",
              "      <th>FLUX.3178</th>\n",
              "      <th>FLUX.3179</th>\n",
              "      <th>FLUX.3180</th>\n",
              "      <th>FLUX.3181</th>\n",
              "      <th>FLUX.3182</th>\n",
              "      <th>FLUX.3183</th>\n",
              "      <th>FLUX.3184</th>\n",
              "      <th>FLUX.3185</th>\n",
              "      <th>FLUX.3186</th>\n",
              "      <th>FLUX.3187</th>\n",
              "      <th>FLUX.3188</th>\n",
              "      <th>FLUX.3189</th>\n",
              "      <th>FLUX.3190</th>\n",
              "      <th>FLUX.3191</th>\n",
              "      <th>FLUX.3192</th>\n",
              "      <th>FLUX.3193</th>\n",
              "      <th>FLUX.3194</th>\n",
              "      <th>FLUX.3195</th>\n",
              "      <th>FLUX.3196</th>\n",
              "      <th>FLUX.3197</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2</td>\n",
              "      <td>119.88</td>\n",
              "      <td>100.21</td>\n",
              "      <td>86.46</td>\n",
              "      <td>48.68</td>\n",
              "      <td>46.12</td>\n",
              "      <td>39.39</td>\n",
              "      <td>18.57</td>\n",
              "      <td>6.98</td>\n",
              "      <td>6.63</td>\n",
              "      <td>-21.97</td>\n",
              "      <td>-23.17</td>\n",
              "      <td>-29.26</td>\n",
              "      <td>-33.99</td>\n",
              "      <td>-6.25</td>\n",
              "      <td>-28.12</td>\n",
              "      <td>-27.24</td>\n",
              "      <td>-32.28</td>\n",
              "      <td>-12.29</td>\n",
              "      <td>-16.57</td>\n",
              "      <td>-23.86</td>\n",
              "      <td>-5.69</td>\n",
              "      <td>9.24</td>\n",
              "      <td>35.52</td>\n",
              "      <td>81.20</td>\n",
              "      <td>116.49</td>\n",
              "      <td>133.99</td>\n",
              "      <td>148.97</td>\n",
              "      <td>174.15</td>\n",
              "      <td>187.77</td>\n",
              "      <td>215.30</td>\n",
              "      <td>246.80</td>\n",
              "      <td>-56.68</td>\n",
              "      <td>-56.68</td>\n",
              "      <td>-56.68</td>\n",
              "      <td>-52.05</td>\n",
              "      <td>-31.52</td>\n",
              "      <td>-31.15</td>\n",
              "      <td>-48.53</td>\n",
              "      <td>-38.93</td>\n",
              "      <td>...</td>\n",
              "      <td>-2.55</td>\n",
              "      <td>12.26</td>\n",
              "      <td>-7.06</td>\n",
              "      <td>-23.53</td>\n",
              "      <td>2.54</td>\n",
              "      <td>30.21</td>\n",
              "      <td>38.87</td>\n",
              "      <td>-22.86</td>\n",
              "      <td>-22.86</td>\n",
              "      <td>-4.37</td>\n",
              "      <td>2.27</td>\n",
              "      <td>-16.27</td>\n",
              "      <td>-30.84</td>\n",
              "      <td>-7.21</td>\n",
              "      <td>-4.27</td>\n",
              "      <td>13.60</td>\n",
              "      <td>15.62</td>\n",
              "      <td>31.96</td>\n",
              "      <td>49.89</td>\n",
              "      <td>86.93</td>\n",
              "      <td>86.93</td>\n",
              "      <td>42.99</td>\n",
              "      <td>48.76</td>\n",
              "      <td>22.82</td>\n",
              "      <td>32.79</td>\n",
              "      <td>30.76</td>\n",
              "      <td>14.55</td>\n",
              "      <td>10.92</td>\n",
              "      <td>22.68</td>\n",
              "      <td>5.91</td>\n",
              "      <td>14.52</td>\n",
              "      <td>19.29</td>\n",
              "      <td>14.44</td>\n",
              "      <td>-1.62</td>\n",
              "      <td>13.33</td>\n",
              "      <td>45.50</td>\n",
              "      <td>31.93</td>\n",
              "      <td>35.78</td>\n",
              "      <td>269.43</td>\n",
              "      <td>57.72</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>5736.59</td>\n",
              "      <td>5699.98</td>\n",
              "      <td>5717.16</td>\n",
              "      <td>5692.73</td>\n",
              "      <td>5663.83</td>\n",
              "      <td>5631.16</td>\n",
              "      <td>5626.39</td>\n",
              "      <td>5569.47</td>\n",
              "      <td>5550.44</td>\n",
              "      <td>5458.80</td>\n",
              "      <td>5329.39</td>\n",
              "      <td>5191.38</td>\n",
              "      <td>5031.39</td>\n",
              "      <td>4769.89</td>\n",
              "      <td>4419.66</td>\n",
              "      <td>4218.92</td>\n",
              "      <td>3924.73</td>\n",
              "      <td>3605.30</td>\n",
              "      <td>3326.55</td>\n",
              "      <td>3021.20</td>\n",
              "      <td>2800.61</td>\n",
              "      <td>2474.48</td>\n",
              "      <td>2258.33</td>\n",
              "      <td>1951.69</td>\n",
              "      <td>1749.86</td>\n",
              "      <td>1585.38</td>\n",
              "      <td>1575.48</td>\n",
              "      <td>1568.41</td>\n",
              "      <td>1661.08</td>\n",
              "      <td>1977.33</td>\n",
              "      <td>2425.62</td>\n",
              "      <td>2889.61</td>\n",
              "      <td>3847.64</td>\n",
              "      <td>3847.64</td>\n",
              "      <td>3741.20</td>\n",
              "      <td>3453.47</td>\n",
              "      <td>3202.61</td>\n",
              "      <td>2923.73</td>\n",
              "      <td>2694.84</td>\n",
              "      <td>...</td>\n",
              "      <td>-3470.75</td>\n",
              "      <td>-4510.72</td>\n",
              "      <td>-5013.41</td>\n",
              "      <td>-3636.05</td>\n",
              "      <td>-2324.27</td>\n",
              "      <td>-2688.55</td>\n",
              "      <td>-2813.66</td>\n",
              "      <td>-586.22</td>\n",
              "      <td>-586.22</td>\n",
              "      <td>-756.80</td>\n",
              "      <td>-1090.23</td>\n",
              "      <td>-1388.61</td>\n",
              "      <td>-1745.36</td>\n",
              "      <td>-2015.28</td>\n",
              "      <td>-2359.06</td>\n",
              "      <td>-2516.66</td>\n",
              "      <td>-2699.31</td>\n",
              "      <td>-2777.55</td>\n",
              "      <td>-2732.97</td>\n",
              "      <td>1167.39</td>\n",
              "      <td>1167.39</td>\n",
              "      <td>1368.89</td>\n",
              "      <td>1434.80</td>\n",
              "      <td>1360.75</td>\n",
              "      <td>1148.44</td>\n",
              "      <td>1117.67</td>\n",
              "      <td>714.86</td>\n",
              "      <td>419.02</td>\n",
              "      <td>57.06</td>\n",
              "      <td>-175.66</td>\n",
              "      <td>-581.91</td>\n",
              "      <td>-984.09</td>\n",
              "      <td>-1230.89</td>\n",
              "      <td>-1600.45</td>\n",
              "      <td>-1824.53</td>\n",
              "      <td>-2061.17</td>\n",
              "      <td>-2265.98</td>\n",
              "      <td>-2366.19</td>\n",
              "      <td>-2294.86</td>\n",
              "      <td>-2034.72</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>844.48</td>\n",
              "      <td>817.49</td>\n",
              "      <td>770.07</td>\n",
              "      <td>675.01</td>\n",
              "      <td>605.52</td>\n",
              "      <td>499.45</td>\n",
              "      <td>440.77</td>\n",
              "      <td>362.95</td>\n",
              "      <td>207.27</td>\n",
              "      <td>150.46</td>\n",
              "      <td>85.49</td>\n",
              "      <td>-20.12</td>\n",
              "      <td>-35.88</td>\n",
              "      <td>-65.59</td>\n",
              "      <td>-15.12</td>\n",
              "      <td>16.60</td>\n",
              "      <td>-25.70</td>\n",
              "      <td>61.88</td>\n",
              "      <td>53.18</td>\n",
              "      <td>64.32</td>\n",
              "      <td>72.38</td>\n",
              "      <td>100.35</td>\n",
              "      <td>67.26</td>\n",
              "      <td>14.71</td>\n",
              "      <td>-16.41</td>\n",
              "      <td>-147.46</td>\n",
              "      <td>-231.27</td>\n",
              "      <td>-320.29</td>\n",
              "      <td>-407.82</td>\n",
              "      <td>-450.48</td>\n",
              "      <td>-146.99</td>\n",
              "      <td>-146.99</td>\n",
              "      <td>-146.99</td>\n",
              "      <td>-146.99</td>\n",
              "      <td>-166.30</td>\n",
              "      <td>-139.90</td>\n",
              "      <td>-96.41</td>\n",
              "      <td>-23.49</td>\n",
              "      <td>13.59</td>\n",
              "      <td>...</td>\n",
              "      <td>-35.24</td>\n",
              "      <td>-70.13</td>\n",
              "      <td>-35.30</td>\n",
              "      <td>-56.48</td>\n",
              "      <td>-74.60</td>\n",
              "      <td>-115.18</td>\n",
              "      <td>-8.91</td>\n",
              "      <td>-37.59</td>\n",
              "      <td>-37.59</td>\n",
              "      <td>-37.43</td>\n",
              "      <td>-104.23</td>\n",
              "      <td>-101.45</td>\n",
              "      <td>-107.35</td>\n",
              "      <td>-109.82</td>\n",
              "      <td>-126.27</td>\n",
              "      <td>-170.32</td>\n",
              "      <td>-117.85</td>\n",
              "      <td>-32.30</td>\n",
              "      <td>-70.18</td>\n",
              "      <td>314.29</td>\n",
              "      <td>314.29</td>\n",
              "      <td>314.29</td>\n",
              "      <td>149.71</td>\n",
              "      <td>54.60</td>\n",
              "      <td>12.60</td>\n",
              "      <td>-133.68</td>\n",
              "      <td>-78.16</td>\n",
              "      <td>-52.30</td>\n",
              "      <td>-8.55</td>\n",
              "      <td>-19.73</td>\n",
              "      <td>17.82</td>\n",
              "      <td>-51.66</td>\n",
              "      <td>-48.29</td>\n",
              "      <td>-59.99</td>\n",
              "      <td>-82.10</td>\n",
              "      <td>-174.54</td>\n",
              "      <td>-95.23</td>\n",
              "      <td>-162.68</td>\n",
              "      <td>-36.79</td>\n",
              "      <td>30.63</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2</td>\n",
              "      <td>-826.00</td>\n",
              "      <td>-827.31</td>\n",
              "      <td>-846.12</td>\n",
              "      <td>-836.03</td>\n",
              "      <td>-745.50</td>\n",
              "      <td>-784.69</td>\n",
              "      <td>-791.22</td>\n",
              "      <td>-746.50</td>\n",
              "      <td>-709.53</td>\n",
              "      <td>-679.56</td>\n",
              "      <td>-706.03</td>\n",
              "      <td>-720.56</td>\n",
              "      <td>-631.12</td>\n",
              "      <td>-659.16</td>\n",
              "      <td>-672.03</td>\n",
              "      <td>-665.06</td>\n",
              "      <td>-667.94</td>\n",
              "      <td>-660.84</td>\n",
              "      <td>-672.75</td>\n",
              "      <td>-644.91</td>\n",
              "      <td>-680.53</td>\n",
              "      <td>-620.50</td>\n",
              "      <td>-570.34</td>\n",
              "      <td>-530.00</td>\n",
              "      <td>-537.88</td>\n",
              "      <td>-578.38</td>\n",
              "      <td>-532.34</td>\n",
              "      <td>-532.38</td>\n",
              "      <td>-491.03</td>\n",
              "      <td>-485.03</td>\n",
              "      <td>-427.19</td>\n",
              "      <td>-380.84</td>\n",
              "      <td>-329.50</td>\n",
              "      <td>-286.91</td>\n",
              "      <td>-283.81</td>\n",
              "      <td>-298.19</td>\n",
              "      <td>-271.03</td>\n",
              "      <td>-268.50</td>\n",
              "      <td>-209.56</td>\n",
              "      <td>...</td>\n",
              "      <td>16.50</td>\n",
              "      <td>-1286.59</td>\n",
              "      <td>-1286.59</td>\n",
              "      <td>-1286.59</td>\n",
              "      <td>-1286.59</td>\n",
              "      <td>-1286.59</td>\n",
              "      <td>-1286.59</td>\n",
              "      <td>-1286.59</td>\n",
              "      <td>-1286.59</td>\n",
              "      <td>-14.94</td>\n",
              "      <td>64.09</td>\n",
              "      <td>8.38</td>\n",
              "      <td>45.31</td>\n",
              "      <td>100.72</td>\n",
              "      <td>91.53</td>\n",
              "      <td>46.69</td>\n",
              "      <td>20.34</td>\n",
              "      <td>30.94</td>\n",
              "      <td>-36.81</td>\n",
              "      <td>-33.28</td>\n",
              "      <td>-69.62</td>\n",
              "      <td>-208.00</td>\n",
              "      <td>-280.28</td>\n",
              "      <td>-340.41</td>\n",
              "      <td>-337.41</td>\n",
              "      <td>-268.03</td>\n",
              "      <td>-245.00</td>\n",
              "      <td>-230.62</td>\n",
              "      <td>-129.59</td>\n",
              "      <td>-35.47</td>\n",
              "      <td>122.34</td>\n",
              "      <td>93.03</td>\n",
              "      <td>93.03</td>\n",
              "      <td>68.81</td>\n",
              "      <td>9.81</td>\n",
              "      <td>20.75</td>\n",
              "      <td>20.25</td>\n",
              "      <td>-120.81</td>\n",
              "      <td>-257.56</td>\n",
              "      <td>-215.41</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "      <td>-39.57</td>\n",
              "      <td>-15.88</td>\n",
              "      <td>-9.16</td>\n",
              "      <td>-6.37</td>\n",
              "      <td>-16.13</td>\n",
              "      <td>-24.05</td>\n",
              "      <td>-0.90</td>\n",
              "      <td>-45.20</td>\n",
              "      <td>-5.04</td>\n",
              "      <td>14.62</td>\n",
              "      <td>-19.52</td>\n",
              "      <td>-11.43</td>\n",
              "      <td>-49.80</td>\n",
              "      <td>25.84</td>\n",
              "      <td>11.62</td>\n",
              "      <td>3.18</td>\n",
              "      <td>-9.59</td>\n",
              "      <td>14.49</td>\n",
              "      <td>8.82</td>\n",
              "      <td>32.32</td>\n",
              "      <td>-28.90</td>\n",
              "      <td>-28.90</td>\n",
              "      <td>-14.09</td>\n",
              "      <td>-30.87</td>\n",
              "      <td>-18.99</td>\n",
              "      <td>-38.60</td>\n",
              "      <td>-27.79</td>\n",
              "      <td>9.65</td>\n",
              "      <td>29.60</td>\n",
              "      <td>7.88</td>\n",
              "      <td>42.87</td>\n",
              "      <td>27.59</td>\n",
              "      <td>27.05</td>\n",
              "      <td>20.26</td>\n",
              "      <td>29.48</td>\n",
              "      <td>9.71</td>\n",
              "      <td>22.84</td>\n",
              "      <td>25.99</td>\n",
              "      <td>-667.55</td>\n",
              "      <td>...</td>\n",
              "      <td>-122.12</td>\n",
              "      <td>-32.01</td>\n",
              "      <td>-47.15</td>\n",
              "      <td>-56.45</td>\n",
              "      <td>-41.71</td>\n",
              "      <td>-34.13</td>\n",
              "      <td>-43.12</td>\n",
              "      <td>-53.63</td>\n",
              "      <td>-53.63</td>\n",
              "      <td>-53.63</td>\n",
              "      <td>-24.29</td>\n",
              "      <td>22.29</td>\n",
              "      <td>25.18</td>\n",
              "      <td>1.84</td>\n",
              "      <td>-22.29</td>\n",
              "      <td>-26.43</td>\n",
              "      <td>-12.12</td>\n",
              "      <td>-33.05</td>\n",
              "      <td>-21.66</td>\n",
              "      <td>-228.32</td>\n",
              "      <td>-228.32</td>\n",
              "      <td>-228.32</td>\n",
              "      <td>-187.35</td>\n",
              "      <td>-166.23</td>\n",
              "      <td>-115.54</td>\n",
              "      <td>-50.18</td>\n",
              "      <td>-37.96</td>\n",
              "      <td>-22.37</td>\n",
              "      <td>-4.74</td>\n",
              "      <td>-35.82</td>\n",
              "      <td>-37.87</td>\n",
              "      <td>-61.85</td>\n",
              "      <td>-27.15</td>\n",
              "      <td>-21.18</td>\n",
              "      <td>-33.76</td>\n",
              "      <td>-85.34</td>\n",
              "      <td>-81.46</td>\n",
              "      <td>-61.98</td>\n",
              "      <td>-69.34</td>\n",
              "      <td>-17.84</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 3198 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   LABEL   FLUX.1   FLUX.2   FLUX.3  ...  FLUX.3194  FLUX.3195  FLUX.3196  FLUX.3197\n",
              "0      2   119.88   100.21    86.46  ...      31.93      35.78     269.43      57.72\n",
              "1      2  5736.59  5699.98  5717.16  ...   -2265.98   -2366.19   -2294.86   -2034.72\n",
              "2      2   844.48   817.49   770.07  ...     -95.23    -162.68     -36.79      30.63\n",
              "3      2  -826.00  -827.31  -846.12  ...      20.25    -120.81    -257.56    -215.41\n",
              "4      2   -39.57   -15.88    -9.16  ...     -81.46     -61.98     -69.34     -17.84\n",
              "\n",
              "[5 rows x 3198 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0d8o7u-_qmuA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Preparing the Dependent and Independent Variables\n",
        "\n",
        "x = data.drop(['LABEL'], axis = 1)\n",
        "y = data['LABEL']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92MvJT1Eq3sU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Standardisation and Normalisation\n",
        "\n",
        "x = normalize(x)\n",
        "scaler = StandardScaler()\n",
        "x = scaler.fit_transform(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRo7gvDltE0I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Code to make all labels Binary\n",
        "temp = []\n",
        "for e in y:\n",
        "  e = e-1\n",
        "  temp.append(e)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1MoYL8ZtYXF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Splitting Dataset Into Training and Validation\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(x, temp, random_state = 0, test_size = 0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j3viu9mNBRsX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let us See How Many Elements Are Present In Each Class\n",
        "\n",
        "count_1 = 0\n",
        "count_0 = 0\n",
        "for e in y_train:\n",
        "  if e == 1:\n",
        "    count_1 = count_1 + 1\n",
        "  elif e == 0:\n",
        "    count_0 = count_0 + 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5pzlh0VICbfJ",
        "colab_type": "code",
        "outputId": "2f92ae54-07b1-4b6d-dd29-1ccf1712c820",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(count_0)\n",
        "print(count_1)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3143\n",
            "25\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0--GmLSCCsb8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Clearly the Dataset is Highly Imbalanced. Hence We need to Perform Synthetic Over Sampling."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvA-zPRtmJhv",
        "colab_type": "text"
      },
      "source": [
        "# Preparing Test Set Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hi3wzriSokB7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_testf = test_data.drop(['LABEL'], axis = 1)\n",
        "y_testf = test_data['LABEL']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "0300f9c7-c5e5-4f5b-e5b1-76c4251ae004",
        "id": "S9gQ-mWLqqfL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x_testf.shape"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(570, 3197)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "73739f46-2260-42df-a175-1e9799d915a7",
        "id": "OfvF75a0qqfY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y_testf.shape"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(570,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "q9auhP4rqqfl",
        "colab": {}
      },
      "source": [
        "x_testf = normalize(x_testf)\n",
        "scaler = StandardScaler()\n",
        "x_testf = scaler.fit_transform(x_testf)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Fati7Qmaqqft",
        "colab": {}
      },
      "source": [
        "# Code to make all labels Binary\n",
        "temp2 = []\n",
        "for e in y_testf:\n",
        "  e = e-1\n",
        "  temp2.append(e)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4BBBHNOC-7E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# x_testf and temp2 are the x_data and y_data for the test set respectively"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q2iUkcLuttjY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Dimentionlity reduction(Optional..In this Experiment Not Performing Reduction Yielded Better Results)\n",
        "'''from sklearn.decomposition import PCA\n",
        "pca = PCA() \n",
        "X_train = pca.fit_transform(X_train)\n",
        "X_test = pca.transform(X_test)\n",
        "total=sum(pca.explained_variance_)\n",
        "k=0\n",
        "current_variance=0\n",
        "while current_variance/total < 0.90:\n",
        "    current_variance += pca.explained_variance_[k]\n",
        "    k=k+1'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rL6NCeUAt9MO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Apply PCA with n_componenets\n",
        "'''pca = PCA(n_components=k)\n",
        "X_train = pca.fit_transform(X_train)\n",
        "X_test = pca.transform(X_test)\n",
        "plt.figure()\n",
        "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
        "plt.xlabel('Number of Features')\n",
        "plt.ylabel('Variance (%)') #for each component\n",
        "plt.title('Variance in Dataset with Features')\n",
        "plt.show()'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7SrIzP1Ak-h",
        "colab_type": "text"
      },
      "source": [
        "# OverSampling Using SMOTE(Synthetic Minority Over-Sampling Technique)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-teivcLv3wp6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Now we use SMOTE to Synthesise Artificial Data to balance classes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wmRK6cw-3V9z",
        "outputId": "6b3ee679-e807-48dd-ed72-b26e5dc475e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "from imblearn.over_sampling import SMOTE"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/six.py:31: FutureWarning: The module is deprecated in version 0.21 and will be removed in version 0.23 since we've dropped support for Python 2.7. Please rely on the official version of six (https://pypi.org/project/six/).\n",
            "  \"(https://pypi.org/project/six/).\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.neighbors.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "c83d23e2-e92b-4f4b-a9c1-2b2c91b19bf2",
        "id": "dKkSYf9n3T-x",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "sm = SMOTE(random_state=27, ratio = 1.0)\n",
        "x_train_res, y_train_res = sm.fit_sample(X_train, y_train)\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1uV9V6z46Q_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's count how many elements are present in Class 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-8uPYqs5A-e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The number of elements in each Class is now Equal\n",
        "\n",
        "count_1 = 0\n",
        "count_0 = 0\n",
        "for e in y_train_res:\n",
        "  if e == 1:\n",
        "    count_1 = count_1 + 1\n",
        "  elif e == 0:\n",
        "    count_0 = count_0 + 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jO26w28LDWvx",
        "colab_type": "code",
        "outputId": "0d885ba2-5405-4eaa-90df-dea77bda3f0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(count_0)\n",
        "print(count_1)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3143\n",
            "3143\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8uOCfZ6N_rK",
        "colab_type": "code",
        "outputId": "ef10db1e-666e-4090-b4e0-8c7eebf4ba84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x_train_res.shape"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6286, 3197)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Asw9XZnYOCCa",
        "colab_type": "code",
        "outputId": "f9bb7403-7e18-4064-9e02-64e741028e51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y_train_res.shape"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6286,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MtoO5VEGYd9x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Changing Input Dimesnions to Make it Usable In CNNs\n",
        "\n",
        "from numpy import newaxis\n",
        "x_train = x_train_res[ :, :, newaxis]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_O2DRvvpZcYO",
        "colab_type": "code",
        "outputId": "2f7af459-3d94-450e-a42f-bbe7c173f3f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x_train.shape"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6286, 3197, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxQM6i2IZhrR",
        "colab_type": "code",
        "outputId": "986614df-7f13-4a3f-96f2-d0532c507a45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "X_test.shape"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(792, 3197)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crCTDqGQZkmr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_test = X_test[ :, :, newaxis]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ZoMBw7IEEbC",
        "colab_type": "code",
        "outputId": "cc46f4d1-9c0d-4d6c-9d7a-eb0440a67690",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x_test.shape"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(792, 3197, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGNha6slrE5O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_testf2 = x_testf[ :, :, newaxis]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMmQsVNWrKsb",
        "colab_type": "code",
        "outputId": "b5f25af7-bb41-483a-89b8-7bea933d37b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x_testf2.shape"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(570, 3197, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGzideCgEJ4L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Note that x_test and y_test are for Validation. x_testf2 and temp2 are Test Set."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sz_pt6gNgSp5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9b6d5b89-0e59-41e2-e049-c670999443b8"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Flatten,Activation, Dropout\n",
        "from keras.layers import Conv1D, MaxPooling1D, GlobalMaxPooling1D"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tAnBGVEb-Pcx",
        "colab_type": "text"
      },
      "source": [
        "# Multi-Channel CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QiLbxMg1-e1S",
        "colab_type": "code",
        "outputId": "85943488-0870-4c14-cfc6-80d266252c7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 765
        }
      },
      "source": [
        "# First Global-View CNN\n",
        "model4 = Sequential(layers=[\n",
        "    # Note the input shape is the desired size of the Signal \n",
        "    # This is the first convolution\n",
        "    Conv1D(3, (2001), activation='relu', input_shape=(3197,1)),\n",
        "    Conv1D(16, (5), activation='relu'),\n",
        "    Conv1D(16, (5), activation='relu'),\n",
        "    MaxPooling1D(5, 2),\n",
        "   \n",
        "    Conv1D(32, (5), activation='relu'),\n",
        "    Conv1D(32, (5), activation='relu'),\n",
        "    MaxPooling1D(5, 2),\n",
        "\n",
        "    Conv1D(64, (5), activation='relu'),\n",
        "    Conv1D(64, (5), activation='relu'),\n",
        "    MaxPooling1D(5, 2),\n",
        "\n",
        "    Conv1D(128, (5), activation='relu'),\n",
        "    Conv1D(128, (5), activation='relu'),\n",
        "    MaxPooling1D(5, 2),\n",
        "\n",
        "    Conv1D(256, (5), activation='relu'),\n",
        "    Conv1D(256, (5), activation='relu'),\n",
        "    MaxPooling1D(5, 2),\n",
        "  \n",
        "    # Flatten the results to feed into a DNN\n",
        "    Flatten(),\n",
        "    Dense(256, activation='relu'),\n",
        "    \n",
        "])\n",
        "\n",
        "model4.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv1d_1 (Conv1D)            (None, 1197, 3)           6006      \n",
            "_________________________________________________________________\n",
            "conv1d_2 (Conv1D)            (None, 1193, 16)          256       \n",
            "_________________________________________________________________\n",
            "conv1d_3 (Conv1D)            (None, 1189, 16)          1296      \n",
            "_________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1 (None, 593, 16)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_4 (Conv1D)            (None, 589, 32)           2592      \n",
            "_________________________________________________________________\n",
            "conv1d_5 (Conv1D)            (None, 585, 32)           5152      \n",
            "_________________________________________________________________\n",
            "max_pooling1d_2 (MaxPooling1 (None, 291, 32)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_6 (Conv1D)            (None, 287, 64)           10304     \n",
            "_________________________________________________________________\n",
            "conv1d_7 (Conv1D)            (None, 283, 64)           20544     \n",
            "_________________________________________________________________\n",
            "max_pooling1d_3 (MaxPooling1 (None, 140, 64)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_8 (Conv1D)            (None, 136, 128)          41088     \n",
            "_________________________________________________________________\n",
            "conv1d_9 (Conv1D)            (None, 132, 128)          82048     \n",
            "_________________________________________________________________\n",
            "max_pooling1d_4 (MaxPooling1 (None, 64, 128)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_10 (Conv1D)           (None, 60, 256)           164096    \n",
            "_________________________________________________________________\n",
            "conv1d_11 (Conv1D)           (None, 56, 256)           327936    \n",
            "_________________________________________________________________\n",
            "max_pooling1d_5 (MaxPooling1 (None, 26, 256)           0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 6656)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 256)               1704192   \n",
            "=================================================================\n",
            "Total params: 2,365,510\n",
            "Trainable params: 2,365,510\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUcmbVwkBR9O",
        "colab_type": "code",
        "outputId": "49906587-5dd8-4448-9aaa-62cafb915003",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "source": [
        "# Local View CNN\n",
        "model5 = Sequential(layers=[\n",
        "    # Note the input shape is the desired size of the signal\n",
        "    # This is the first convolution\n",
        "    Conv1D(3, (201), activation='relu', input_shape=(3197,1)),\n",
        "    Conv1D(16, (5), activation='relu'),\n",
        "    Conv1D(16, (5), activation='relu'),\n",
        "    MaxPooling1D(7, 2),\n",
        "   \n",
        "    Conv1D(32, (5), activation='relu'),\n",
        "    Conv1D(32, (5), activation='relu'),\n",
        "    MaxPooling1D(7, 2),\n",
        "\n",
        "  \n",
        "    # Flatten the results to feed into a DNN\n",
        "    Flatten(),\n",
        "    Dense(256, activation='relu'),\n",
        "\n",
        "    \n",
        "])\n",
        "\n",
        "model5.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv1d_12 (Conv1D)           (None, 2997, 3)           606       \n",
            "_________________________________________________________________\n",
            "conv1d_13 (Conv1D)           (None, 2993, 16)          256       \n",
            "_________________________________________________________________\n",
            "conv1d_14 (Conv1D)           (None, 2989, 16)          1296      \n",
            "_________________________________________________________________\n",
            "max_pooling1d_6 (MaxPooling1 (None, 1492, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv1d_15 (Conv1D)           (None, 1488, 32)          2592      \n",
            "_________________________________________________________________\n",
            "conv1d_16 (Conv1D)           (None, 1484, 32)          5152      \n",
            "_________________________________________________________________\n",
            "max_pooling1d_7 (MaxPooling1 (None, 739, 32)           0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 23648)             0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 256)               6054144   \n",
            "=================================================================\n",
            "Total params: 6,064,046\n",
            "Trainable params: 6,064,046\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "znbLc2p8BpUj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Merging outputs of both Models Together\n",
        "\n",
        "from keras.layers import concatenate\n",
        "from keras.layers import *\n",
        "mergedOut = Add()([model4.output,model5.output])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghq-Z48tKelF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Adding Some Fully-Connected Layers\n",
        "\n",
        "mergedOut = Dense(512, activation='relu')(mergedOut)\n",
        "mergedOut = Dense(512, activation='relu')(mergedOut)\n",
        "mergedOut = Dense(512, activation='relu')(mergedOut)\n",
        "\n",
        "# output layer\n",
        "mergedOut = Dense(1, activation='sigmoid')(mergedOut)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2m01V9JeK0t9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Model\n",
        "\n",
        "newModel = Model([model4.input,model5.input], mergedOut)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7GhdejULyPx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "adam = keras.optimizers.Adadelta(learning_rate=0.001, rho=0.95)\n",
        "newModel.compile(loss='binary_crossentropy',\n",
        "               optimizer = adam,\n",
        "               metrics=['acc'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4hBS7sSUK_IL",
        "colab_type": "code",
        "outputId": "3800d3fa-fdcc-4301-f97a-7cb4c6f6cbe5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        }
      },
      "source": [
        "history4 = newModel.fit([x_train,x_train], y_train_res, epochs = 20, batch_size = 20, validation_data = ([x_test,x_test],y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 6286 samples, validate on 792 samples\n",
            "Epoch 1/20\n",
            "6286/6286 [==============================] - 7s 1ms/step - loss: 0.1313 - acc: 0.9739 - val_loss: 0.1571 - val_acc: 0.9356\n",
            "Epoch 2/20\n",
            "6286/6286 [==============================] - 7s 1ms/step - loss: 0.1133 - acc: 0.9792 - val_loss: 0.1420 - val_acc: 0.9432\n",
            "Epoch 3/20\n",
            "6286/6286 [==============================] - 7s 1ms/step - loss: 0.0985 - acc: 0.9809 - val_loss: 0.1263 - val_acc: 0.9495\n",
            "Epoch 4/20\n",
            "6286/6286 [==============================] - 7s 1ms/step - loss: 0.0864 - acc: 0.9833 - val_loss: 0.1374 - val_acc: 0.9444\n",
            "Epoch 5/20\n",
            "6286/6286 [==============================] - 7s 1ms/step - loss: 0.0764 - acc: 0.9844 - val_loss: 0.1192 - val_acc: 0.9533\n",
            "Epoch 6/20\n",
            "6286/6286 [==============================] - 7s 1ms/step - loss: 0.0677 - acc: 0.9866 - val_loss: 0.1003 - val_acc: 0.9646\n",
            "Epoch 7/20\n",
            "6286/6286 [==============================] - 7s 1ms/step - loss: 0.0602 - acc: 0.9882 - val_loss: 0.0918 - val_acc: 0.9684\n",
            "Epoch 8/20\n",
            "6286/6286 [==============================] - 7s 1ms/step - loss: 0.0544 - acc: 0.9890 - val_loss: 0.0978 - val_acc: 0.9672\n",
            "Epoch 9/20\n",
            "6286/6286 [==============================] - 7s 1ms/step - loss: 0.0495 - acc: 0.9908 - val_loss: 0.0914 - val_acc: 0.9697\n",
            "Epoch 10/20\n",
            "6286/6286 [==============================] - 7s 1ms/step - loss: 0.0450 - acc: 0.9908 - val_loss: 0.0825 - val_acc: 0.9760\n",
            "Epoch 11/20\n",
            "6286/6286 [==============================] - 7s 1ms/step - loss: 0.0415 - acc: 0.9924 - val_loss: 0.0792 - val_acc: 0.9773\n",
            "Epoch 12/20\n",
            "6286/6286 [==============================] - 7s 1ms/step - loss: 0.0383 - acc: 0.9924 - val_loss: 0.0758 - val_acc: 0.9811\n",
            "Epoch 13/20\n",
            "6286/6286 [==============================] - 7s 1ms/step - loss: 0.0356 - acc: 0.9933 - val_loss: 0.0803 - val_acc: 0.9773\n",
            "Epoch 14/20\n",
            "6286/6286 [==============================] - 7s 1ms/step - loss: 0.0333 - acc: 0.9940 - val_loss: 0.0771 - val_acc: 0.9798\n",
            "Epoch 15/20\n",
            "6286/6286 [==============================] - 7s 1ms/step - loss: 0.0311 - acc: 0.9938 - val_loss: 0.0765 - val_acc: 0.9811\n",
            "Epoch 16/20\n",
            "6286/6286 [==============================] - 7s 1ms/step - loss: 0.0291 - acc: 0.9943 - val_loss: 0.0746 - val_acc: 0.9836\n",
            "Epoch 17/20\n",
            "6286/6286 [==============================] - 7s 1ms/step - loss: 0.0273 - acc: 0.9943 - val_loss: 0.0711 - val_acc: 0.9836\n",
            "Epoch 18/20\n",
            "6286/6286 [==============================] - 7s 1ms/step - loss: 0.0259 - acc: 0.9948 - val_loss: 0.0737 - val_acc: 0.9823\n",
            "Epoch 19/20\n",
            "6286/6286 [==============================] - 7s 1ms/step - loss: 0.0244 - acc: 0.9946 - val_loss: 0.0716 - val_acc: 0.9836\n",
            "Epoch 20/20\n",
            "6286/6286 [==============================] - 7s 1ms/step - loss: 0.0232 - acc: 0.9951 - val_loss: 0.0701 - val_acc: 0.9836\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "7885b793-dda7-49e8-f609-885332b304ef",
        "id": "tRhwZVUiMxiG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "acc = history4.history['acc']\n",
        "val_acc = history4.history['val_acc']\n",
        "loss = history4.history['loss']\n",
        "val_loss = history4.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'r', label='Training accuracy')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, 'r', label='Training Loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation Loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6wMPf5ohVQ-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We can Observe from Loss and Accuracy Graphs that Model isn't Overtfitting"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "v88KjQEgMxiP",
        "colab": {}
      },
      "source": [
        "y_pred = newModel.predict([x_test,x_test])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "N6osccHlMxiT",
        "colab": {}
      },
      "source": [
        "y_pred2 = []\n",
        "for m in y_pred:\n",
        "  if m > 0.5:\n",
        "    y_pred2.append(1)\n",
        "  else:\n",
        "    y_pred2.append(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "X7R8a1G5MxiW",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "344478c8-e608-43f3-bf64-5e3df009db9e",
        "id": "-raK_xjBMxiZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# The Architecture Identifies 777/784 cases of Non-Exo-Planets Correctly in Validation Set\n",
        "# The Architecture however Fails to Identify Exo-Planet and Identifies 2/8 Cases in Validation Set\n",
        "\n",
        "confusion_matrix(y_pred2,y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[777,   6],\n",
              "       [  7,   2]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "6b93d5d5-af9c-4bed-c872-4b97244283d7",
        "id": "X3KZNd_wMxic",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "print(classification_report(y_test,y_pred2))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.99      0.99       784\n",
            "           1       0.22      0.25      0.24         8\n",
            "\n",
            "    accuracy                           0.98       792\n",
            "   macro avg       0.61      0.62      0.61       792\n",
            "weighted avg       0.98      0.98      0.98       792\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XUUKg28xO6lz",
        "colab": {}
      },
      "source": [
        "y_pred2 = newModel.predict([x_testf2,x_testf2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "A5BJXIEoO6mX",
        "colab": {}
      },
      "source": [
        "y_pred3 = []\n",
        "for m in y_pred2:\n",
        "  if m > 0.5:\n",
        "    y_pred3.append(1)\n",
        "  else:\n",
        "    y_pred3.append(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "1a5b6af9-3ed5-437c-b918-7a0722bbc08d",
        "id": "3jkzIrwvO6md",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# The Architecture Gives good result on Test Set\n",
        "# The Architecture Identifies all Non-ExoPlanets Correctly Which is Extremely Encouraging\n",
        "# The Architecture Identifies 3/5 Exo-Planets Correctly\n",
        "\n",
        "confusion_matrix(y_pred3, temp2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[565,   2],\n",
              "       [  0,   3]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "034e9f46-eace-41d2-8d3e-6cc4b61919fa",
        "id": "wCDY0uKoO6mk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "# Precision and Recall are perfect for the Non-ExoPlanets because the model predicts Non-ExoPlanets Flawlessly\n",
        "# Recall goes down for Exo-Planets because 2 Exo-Planets were Mis-Classified as Non-ExoPlanets\n",
        "\n",
        "print(classification_report(temp2,y_pred3))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       565\n",
            "           1       1.00      0.60      0.75         5\n",
            "\n",
            "    accuracy                           1.00       570\n",
            "   macro avg       1.00      0.80      0.87       570\n",
            "weighted avg       1.00      1.00      1.00       570\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}